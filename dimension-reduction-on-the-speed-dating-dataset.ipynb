{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nyagami/feature-selection-techniques?scriptVersionId=129269677\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# <span style=\"font-family:monospace\">1. Introduction</span>\n## <span style=\"font-family:monospace\">1.1 About the Dataset</span>\n\n<div style = \"font-family:georgia, serif; font-size:17px\">\n\nThe dataset is retrived from <a href = \"https://www.kaggle.com/datasets/ulrikthygepedersen/speed-dating\">Ulrik Thyge Pedersen</a> on Kaggle. \n\nThis data was gathered from participants in experimental speed dating events from 2002-2004. During the events, the attendees would have a four-minute \"first date\" with every other participant of the opposite sex. At the end of their four minutes, participants were asked if they would like to see their date again. They were also asked to rate their date on six attributes: Attractiveness, Sincerity, Intelligence, Fun, Ambition, and Shared Interests. The dataset also includes questionnaire data gathered from participants at different points in the process. These fields include: demographics, dating habits, self-perception across key attributes, beliefs on what others find valuable in a mate, and lifestyle information.\n\n<table>\n    <tbody>\n        <tr><th>Variable</th><th>Description</th></tr>\n        <tr><td>gender</td><td> Gender of self</td></tr>\n        <tr><td>age</td><td> Age of self</td></tr>\n        <tr><td>age_o</td><td> Age of partner</td></tr>\n        <tr><td>d_age</td><td> Difference in age</td></tr>\n        <tr><td>d_age</td><td> Difference in age</td></tr>\n        <tr><td>race</td><td> Race of self</td></tr>\n        <tr><td>race_o</td><td> Race of partner</td></tr>\n        <tr><td>samerace</td><td> Whether the two persons have the same race or not.</td></tr>\n        <tr><td>importance_same_race</td><td> How important is it that partner is of same race?</td></tr>\n        <tr><td>importance_same_religion</td><td> How important is it that partner has same religion?</td></tr>\n        <tr><td>field</td><td> Field of study</td></tr>\n        <tr><td>pref_o_attractive</td><td> How important does partner rate attractiveness</td></tr>\n        <tr><td>pref_o_sinsere</td><td> How important does partner rate sincerity</td></tr>\n        <tr><td>pref_o_intelligence</td><td> How important does partner rate intelligence</td></tr>\n        <tr><td>pref_o_funny</td><td> How important does partner rate being funny</td></tr>\n        <tr><td>pref_o_ambitious</td><td> How important does partner rate ambition</td></tr>\n        <tr><td>pref_o_shared_interests</td><td> How important does partner rate having shared interests</td></tr>\n        <tr><td>attractive_o</td><td> Rating by partner (about me) at night of event on attractiveness</td></tr>\n        <tr><td>sincere_o</td><td> Rating by partner (about me) at night of event on sincerity</td></tr>\n        <tr><td>intelligence_o</td><td> Rating by partner (about me) at night of event on intelligence</td></tr>\n        <tr><td>funny_o</td><td> Rating by partner (about me) at night of event on being funny</td></tr>\n        <tr><td>ambitous_o</td><td> Rating by partner (about me) at night of event on being ambitious</td></tr>\n        <tr><td>shared_interests_o</td><td> Rating by partner (about me) at night of event on shared interest</td></tr>\n        <tr><td>attractive_important</td><td> What do you look for in a partner - attractiveness</td></tr>\n        <tr><td>sincere_important</td><td> What do you look for in a partner - sincerity</td></tr>\n        <tr><td>intellicence_important</td><td> What do you look for in a partner - intelligence</td></tr>\n        <tr><td>funny_important</td><td> What do you look for in a partner - being funny</td></tr>\n        <tr><td>ambtition_important</td><td> What do you look for in a partner - ambition</td></tr>\n        <tr><td>shared_interests_important</td><td> What do you look for in a partner - shared interests</td></tr>\n        <tr><td>attractive</td><td> Rate yourself - attractiveness</td></tr>\n        <tr><td>sincere</td><td> Rate yourself - sincerity</td></tr>\n        <tr><td>intelligence</td><td> Rate yourself - intelligence</td></tr>\n        <tr><td>funny</td><td> Rate yourself - being funny</td></tr>\n        <tr><td>ambition</td><td> Rate yourself - ambition</td></tr>\n        <tr><td>attractive_partner</td><td> Rate your partner - attractiveness</td></tr>\n        <tr><td>sincere_partner</td><td> Rate your partner - sincerity</td></tr>\n        <tr><td>intelligence_partner</td><td> Rate your partner - intelligence</td></tr>\n        <tr><td>funny_partner</td><td> Rate your partner - being funny</td></tr>\n        <tr><td>ambition_partner</td><td> Rate your partner - ambition</td></tr>\n        <tr><td>shared_interests_partner</td><td> Rate your partner - shared interests</td></tr>\n        <tr><td>sports</td><td> Your own interests [1-10]</td></tr>\n        <tr><td>interests_correlate</td>\n            <td> Correlation between participant’s and partner’s ratings of interests.</td></tr>\n        <tr><td>expected_happy_with_sd_people</td><td> How happy do you expect to be with the people you meet during the speed-dating event?</td></tr>\n        <tr><td>expected_num_interested_in_me</td><td> Out of the 20 people you will meet, how many do you expect will be interested in dating you?</td></tr>\n        <tr><td>expected_num_matches</td><td> How many matches do you expect to get?</td></tr>\n        <tr><td>like</td><td> Did you like your partner?</td></tr>\n        <tr><td>guess_prob_liked</td><td> How likely do you think it is that your partner likes you?</td></tr>\n        <tr><td>met</td><td> Have you met your partner before?</td></tr>\n        <tr><td>decision</td><td> Decision at night of event.</td></tr>\n        <tr><td>decision_o</td><td> Decision of partner at night of event.</td></tr>\n        <tr><td>match</td><td> Match (yes/no)</td></tr>\n    </tbody>\n</table>\n    \n ## <span style=\"font-family:monospace\">1.2 Purpose</span>\n<ul>\n    <li>To clean the dataset and make it ready for a classification problem to determine whether the speed date is a match or not</li>\n<li>Conduct feature selection and extraction using various techniques to find the most important features to predict whether the date is a match or not</li>\n</ul>\n    \n## <span style=\"font-family:monospace\">1.3 Further insight</span>\n    \n<p style = \"font-family:georgia, serif; font-size:17px\">I have written an article explaining the steps followed in dimension reduction of the dataset and it can be found <a href =\"https://thesilentguru.com/dimension-reduction-in-python/\" >HERE</a></p>\n    \n    \n</div> ","metadata":{"_kg_hide-output":false}},{"cell_type":"markdown","source":"# <span style=\"font-family:monospace\">2. Import libraries and read files</span>","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_extraction import DictVectorizer\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nimport xgboost as xgb\n\nimport re\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-27T09:52:13.844597Z","iopub.execute_input":"2023-02-27T09:52:13.845043Z","iopub.status.idle":"2023-02-27T09:52:13.864646Z","shell.execute_reply.started":"2023-02-27T09:52:13.845008Z","shell.execute_reply":"2023-02-27T09:52:13.863046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set default options\npd.set_option('display.max_columns', 150)\npd.set_option('display.max_rows', 130)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:13.967164Z","iopub.execute_input":"2023-02-27T09:52:13.967597Z","iopub.status.idle":"2023-02-27T09:52:13.973966Z","shell.execute_reply.started":"2023-02-27T09:52:13.967564Z","shell.execute_reply":"2023-02-27T09:52:13.972622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read dataset\ndating = pd.read_csv('/kaggle/input/speed-dating/speeddating.csv')","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:14.078646Z","iopub.execute_input":"2023-02-27T09:52:14.079087Z","iopub.status.idle":"2023-02-27T09:52:14.285437Z","shell.execute_reply.started":"2023-02-27T09:52:14.079053Z","shell.execute_reply":"2023-02-27T09:52:14.284023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"font-family:monospace\">3.Preview file</span>","metadata":{}},{"cell_type":"code","source":"dating.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:14.288317Z","iopub.execute_input":"2023-02-27T09:52:14.288734Z","iopub.status.idle":"2023-02-27T09:52:14.407539Z","shell.execute_reply.started":"2023-02-27T09:52:14.2887Z","shell.execute_reply":"2023-02-27T09:52:14.40627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dating.info()","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:14.40932Z","iopub.execute_input":"2023-02-27T09:52:14.410238Z","iopub.status.idle":"2023-02-27T09:52:14.433181Z","shell.execute_reply.started":"2023-02-27T09:52:14.410187Z","shell.execute_reply":"2023-02-27T09:52:14.431114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dating.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:14.436054Z","iopub.execute_input":"2023-02-27T09:52:14.436417Z","iopub.status.idle":"2023-02-27T09:52:14.44432Z","shell.execute_reply.started":"2023-02-27T09:52:14.436387Z","shell.execute_reply":"2023-02-27T09:52:14.442945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"font-family:monospace\">4. Data preprocessing</span>\n## <span style=\"font-family:monospace; margin-left: 25px\">4.1 Checking column data types</span>","metadata":{}},{"cell_type":"code","source":"dating.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:14.445732Z","iopub.execute_input":"2023-02-27T09:52:14.446186Z","iopub.status.idle":"2023-02-27T09:52:14.461649Z","shell.execute_reply.started":"2023-02-27T09:52:14.446149Z","shell.execute_reply":"2023-02-27T09:52:14.460196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"font-family:monospace; margin-left: 25px\">4.2 Removing unwanted characters</span>\n<p style = \"font-family:georgia, serif; font-size:17px\">\nAll string columns have an unwanted character 'b' and are enclosed by single quotes. We will remove these characters from the columns.</p>","metadata":{}},{"cell_type":"code","source":"dating.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:14.463401Z","iopub.execute_input":"2023-02-27T09:52:14.463743Z","iopub.status.idle":"2023-02-27T09:52:14.57277Z","shell.execute_reply.started":"2023-02-27T09:52:14.463714Z","shell.execute_reply":"2023-02-27T09:52:14.571413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We create a function that removes all the unwanted characters\ndef remove_xters(feature):\n    return feature.replace(\"b'\",'').replace(\"'\",\"\")\n\n# We select string columns and apply the transformation\nstring_dataset = dating.select_dtypes(include = ['object'])\n\nfor feature in string_dataset.columns:\n    dating[feature] = dating[feature].apply(lambda x: remove_xters(x))\n    \ndating[string_dataset.columns].head(3)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:14.574273Z","iopub.execute_input":"2023-02-27T09:52:14.574626Z","iopub.status.idle":"2023-02-27T09:52:15.030843Z","shell.execute_reply.started":"2023-02-27T09:52:14.574595Z","shell.execute_reply":"2023-02-27T09:52:15.02965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the has_null and wave columns\ndating.drop(['has_null','wave'], axis = 1, inplace= True)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:15.032499Z","iopub.execute_input":"2023-02-27T09:52:15.033166Z","iopub.status.idle":"2023-02-27T09:52:15.048019Z","shell.execute_reply.started":"2023-02-27T09:52:15.033126Z","shell.execute_reply":"2023-02-27T09:52:15.046613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-family:georgia, serif; font-size:17px\">The same race variable is a string data type. We convert it to numeric</p>","metadata":{}},{"cell_type":"code","source":"dating['samerace'] = dating['samerace'].astype('int')","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:15.052482Z","iopub.execute_input":"2023-02-27T09:52:15.052897Z","iopub.status.idle":"2023-02-27T09:52:15.082212Z","shell.execute_reply.started":"2023-02-27T09:52:15.052861Z","shell.execute_reply":"2023-02-27T09:52:15.080391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"font-family:monospace; margin-left: 25px\">4.3 Dropping columns</span>\n### <span style=\"font-family:monospace; margin-left: 25px\"> 4.3.1 Age</span>\n\n\n<p style = \"font-family:georgia, serif; font-size:17px\">\nThere are four different columns for age and these are: age of individual, age of partner, age difference, and age group (where ages have been binned into categories). However the age difference column is not standardized since the difference is between the higher age and the lower age. We will standardize the difference by subtracting age between each individual and their partner. Since the direction of the difference matters, the column will have both positive and negative values. We will then drop the other age columns</p>","metadata":{}},{"cell_type":"code","source":"# Create age difference column\ndating['age_diff'] = dating['age'] - dating['age_o']\n\n# Drop other age columns\ndating.drop(['age','age_o', 'd_age', 'd_d_age'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:15.084219Z","iopub.execute_input":"2023-02-27T09:52:15.084726Z","iopub.status.idle":"2023-02-27T09:52:15.107336Z","shell.execute_reply.started":"2023-02-27T09:52:15.084693Z","shell.execute_reply":"2023-02-27T09:52:15.106187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"font-family:monospace; margin-left: 25px\">4.3.2 Binned variables</span>\n<p style = \"font-family:georgia, serif; font-size:17px\">\nThe speed dating dataset has some of the numeric features binned into categories. This is a duplication of information which increases the dimensionality of the dataset without adding any significant value. We will remove all the duplicated features. Conveniently, the names of these binned features have the prefix \"d_\". We will use this to subset and remove these features. Before the selection, the dataset has 118 features, and after the selection, we remain with 64 features.</p>","metadata":{}},{"cell_type":"code","source":"dating.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:15.108887Z","iopub.execute_input":"2023-02-27T09:52:15.109316Z","iopub.status.idle":"2023-02-27T09:52:15.118188Z","shell.execute_reply.started":"2023-02-27T09:52:15.109283Z","shell.execute_reply":"2023-02-27T09:52:15.116573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_drop = [column_name for column_name in dating.columns if column_name.startswith('d_')]\ndating.drop(to_drop, axis = 1, inplace = True)\ndating.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:15.120073Z","iopub.execute_input":"2023-02-27T09:52:15.120522Z","iopub.status.idle":"2023-02-27T09:52:15.138078Z","shell.execute_reply.started":"2023-02-27T09:52:15.120485Z","shell.execute_reply":"2023-02-27T09:52:15.136207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ### <span style=\"font-family:monospace; margin-left: 25px\">4.3.3 Field</span>\n<p style = \"font-family:georgia, serif; font-size:17px\">\nIn the individual field indicating their careers, there's an overlapping of values. For instance, Business, which is the most popular field still comprises of MBA (2nd most popular), Finance, business [MBA] etc. Physics, Chemistry, Biology all fall under science. Due to this variation and overlapping, we will drop the column</p>","metadata":{"execution":{"iopub.status.busy":"2023-02-22T10:31:41.782896Z","iopub.execute_input":"2023-02-22T10:31:41.783299Z","iopub.status.idle":"2023-02-22T10:31:41.790498Z","shell.execute_reply.started":"2023-02-22T10:31:41.783268Z","shell.execute_reply":"2023-02-22T10:31:41.789438Z"}}},{"cell_type":"code","source":"display(dating.field.value_counts().head(30))\ndating.drop('field',axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:15.141049Z","iopub.execute_input":"2023-02-27T09:52:15.142698Z","iopub.status.idle":"2023-02-27T09:52:15.164402Z","shell.execute_reply.started":"2023-02-27T09:52:15.142609Z","shell.execute_reply":"2023-02-27T09:52:15.162805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"font-family:monospace; margin-left: 25px\">4.3.4 Met</span>\n<p style = \"font-family:georgia, serif; font-size:17px\">\nThe variable met answers the question as to whether the person has previously met the partner. Since the responses are either yes(1) or no (0), we will clean the column and change responses with neither values, and replace them with the most frequent value. Since they are only a few values, they will not skew our data.</p>","metadata":{}},{"cell_type":"code","source":"print(f'Before \\n{dating.met.value_counts()}')\n\nfor number in [3.0, 5.0, 6.0, 7.0, 8.0]:\n    dating['met'].replace(number,0, inplace =True)\n    \nprint(f'\\nAfter \\n{dating.met.value_counts()}')","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:15.166825Z","iopub.execute_input":"2023-02-27T09:52:15.167273Z","iopub.status.idle":"2023-02-27T09:52:15.181972Z","shell.execute_reply.started":"2023-02-27T09:52:15.167238Z","shell.execute_reply":"2023-02-27T09:52:15.180345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"font-family:monospace; margin-left: 25px\">4.3.5 Variables similar to the y variable</span>\n<p style = \"font-family:georgia, serif; font-size:17px\">The two variables 'decision' and 'decision_o' have similar responses almost similar to the variable being predicted. We will therefore remove these two features.</p>","metadata":{}},{"cell_type":"code","source":"display(dating.groupby('decision')['match'].value_counts())\nprint()\ndisplay(dating.groupby('decision_o')['match'].value_counts())\n\ndating.drop(['decision_o','decision'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:15.184157Z","iopub.execute_input":"2023-02-27T09:52:15.185461Z","iopub.status.idle":"2023-02-27T09:52:15.211892Z","shell.execute_reply.started":"2023-02-27T09:52:15.185409Z","shell.execute_reply":"2023-02-27T09:52:15.211026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"font-family:monospace; margin-left: 25px\">4.4 Numeric and categorical columns</span>\n","metadata":{"execution":{"iopub.status.busy":"2023-02-27T05:39:01.585535Z","iopub.execute_input":"2023-02-27T05:39:01.586028Z","iopub.status.idle":"2023-02-27T05:39:01.593068Z","shell.execute_reply.started":"2023-02-27T05:39:01.58599Z","shell.execute_reply":"2023-02-27T05:39:01.591328Z"}}},{"cell_type":"code","source":"# Selecting numeric columns\ncolumns_numeric = dating.select_dtypes(include = ['int','float']).columns.tolist()\n\n# Selecting categorical columns\ncolumns_category = dating.select_dtypes(include = ['object']).drop('match', axis=1).columns","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:15.213278Z","iopub.execute_input":"2023-02-27T09:52:15.214445Z","iopub.status.idle":"2023-02-27T09:52:15.2283Z","shell.execute_reply.started":"2023-02-27T09:52:15.214391Z","shell.execute_reply":"2023-02-27T09:52:15.226925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"font-family:monospace; margin-left: 25px\">4.5. Missing values</span>\n<p style = \"font-family:georgia, serif; font-size:17px\">More than 78% of values in the variable \"expected_num_interested_in_me\" are missing. This variable will therefore be dropped. The rest of the features will be imputed with the median value of each feature.</p>","metadata":{}},{"cell_type":"code","source":"# Show the missing values\ndisplay(dating[columns_numeric].isna().sum()/len(dating)*100)\n\n# Drop the variable with lots of missing values from the dataset\ndating.drop('expected_num_interested_in_me', axis = 1, inplace = True)\n\n# Delete it from the numeric column list\n\ncolumns_numeric.remove('expected_num_interested_in_me')","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:15.230212Z","iopub.execute_input":"2023-02-27T09:52:15.230724Z","iopub.status.idle":"2023-02-27T09:52:15.256282Z","shell.execute_reply.started":"2023-02-27T09:52:15.230678Z","shell.execute_reply":"2023-02-27T09:52:15.254742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the clean dataset for future analysis\ndating.to_csv('/kaggle/working/speed_dating_cleaned.csv')","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:15.257763Z","iopub.execute_input":"2023-02-27T09:52:15.258173Z","iopub.status.idle":"2023-02-27T09:52:15.570593Z","shell.execute_reply.started":"2023-02-27T09:52:15.25814Z","shell.execute_reply":"2023-02-27T09:52:15.568717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"font-family:monospace; margin-left: 25px\">5. Split dataset into training and test set</span>","metadata":{}},{"cell_type":"code","source":"X = dating.drop('match', axis = 1)\ny = dating['match'].astype('int')","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:15.572517Z","iopub.execute_input":"2023-02-27T09:52:15.573049Z","iopub.status.idle":"2023-02-27T09:52:15.584714Z","shell.execute_reply.started":"2023-02-27T09:52:15.573002Z","shell.execute_reply":"2023-02-27T09:52:15.58285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 33 )","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:15.58741Z","iopub.execute_input":"2023-02-27T09:52:15.588011Z","iopub.status.idle":"2023-02-27T09:52:15.602982Z","shell.execute_reply.started":"2023-02-27T09:52:15.587958Z","shell.execute_reply":"2023-02-27T09:52:15.601395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"font-family:monospace\">6. Imputing, scaling and encoding</span>\n## <span style=\"font-family:monospace; margin-left: 25px\">6.1 Numeric variables</span>\n<p style = \"font-family:georgia, serif; font-size:17px\">Numeric variables will be imputed by filling missing values with the median value and standardized using the standard scaler.</p>","metadata":{}},{"cell_type":"code","source":"# Instantiate the numeric imputer and scaler\nnum_imp = SimpleImputer(strategy = 'median')\nscaler = StandardScaler()","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:15.608644Z","iopub.execute_input":"2023-02-27T09:52:15.60914Z","iopub.status.idle":"2023-02-27T09:52:15.614527Z","shell.execute_reply.started":"2023-02-27T09:52:15.609102Z","shell.execute_reply":"2023-02-27T09:52:15.613299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute the training set\nX_train_imp = num_imp.fit_transform(X_train[columns_numeric])\n\n# Scale the training set\nX_train_imp_scaled = scaler.fit_transform(X_train_imp)\n\n# Convert the training set to a dataframe\nX_train_num_final = pd.DataFrame(X_train_imp_scaled, columns = columns_numeric)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:15.616317Z","iopub.execute_input":"2023-02-27T09:52:15.61707Z","iopub.status.idle":"2023-02-27T09:52:15.683568Z","shell.execute_reply.started":"2023-02-27T09:52:15.617024Z","shell.execute_reply":"2023-02-27T09:52:15.682045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute the test set\nX_test_imp = num_imp.fit_transform(X_test[columns_numeric])\n\n# Scale the test set\nX_test_imp_scaled = scaler.fit_transform(X_test_imp)\n\n# Convert the test set to a dataframe\nX_test_num_final = pd.DataFrame(X_test_imp_scaled, columns = columns_numeric)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:15.685285Z","iopub.execute_input":"2023-02-27T09:52:15.685689Z","iopub.status.idle":"2023-02-27T09:52:15.71286Z","shell.execute_reply.started":"2023-02-27T09:52:15.685654Z","shell.execute_reply":"2023-02-27T09:52:15.711114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"font-family:monospace; margin-left: 25px\">6.2 Categorical variables</span>\n<p style = \"font-family:georgia, serif; font-size:17px\">Categorical variables will be imputed by filling missing values with the value \"Unknown\". The variables will then be encoded into numeric values using the dictvectorizer.</p>","metadata":{}},{"cell_type":"code","source":"# Instantiate the categorical imputer and the dictvectorizer\ncat_imp = SimpleImputer(missing_values = '?', fill_value = 'Unknown', strategy = 'constant')\nvectorizer = DictVectorizer(sparse = False)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:15.715428Z","iopub.execute_input":"2023-02-27T09:52:15.716095Z","iopub.status.idle":"2023-02-27T09:52:15.723697Z","shell.execute_reply.started":"2023-02-27T09:52:15.716014Z","shell.execute_reply":"2023-02-27T09:52:15.722099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute the training set\nX_train_imp = cat_imp.fit_transform(X_train[columns_category])\n\n# Convert to dataframe\nX_train_imp_df = pd.DataFrame(X_train_imp, columns = columns_category)\n\n# Encode the training set \nX_train_cat_vect = vectorizer.fit_transform(X_train_imp_df[columns_category].to_dict('records'))\n\n# Convert to dataframe\nX_train_cat_final = pd.DataFrame(X_train_cat_vect, columns = vectorizer.get_feature_names_out())","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:15.725246Z","iopub.execute_input":"2023-02-27T09:52:15.725642Z","iopub.status.idle":"2023-02-27T09:52:15.813839Z","shell.execute_reply.started":"2023-02-27T09:52:15.725607Z","shell.execute_reply":"2023-02-27T09:52:15.812802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute the test set\nX_test_imp = cat_imp.fit_transform(X_test[columns_category])\n\n# Convert to dataframe\nX_test_imp_df = pd.DataFrame(X_test_imp, columns = columns_category)\n\n# Encode the test set \nX_test_cat_vect = vectorizer.fit_transform(X_test_imp_df[columns_category].to_dict('records'))\n\n# Convert to dataframe\nX_test_cat_final = pd.DataFrame(X_test_cat_vect, columns = vectorizer.get_feature_names_out())","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:15.814889Z","iopub.execute_input":"2023-02-27T09:52:15.815224Z","iopub.status.idle":"2023-02-27T09:52:15.854885Z","shell.execute_reply.started":"2023-02-27T09:52:15.815196Z","shell.execute_reply":"2023-02-27T09:52:15.852599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"font-family:monospace; margin-left: 25px\">6.3 Join numeric and categorical variables</span>\n<p style = \"font-family:georgia, serif; font-size:17px\">The transformed numeric and categorical variables are joined together to form the training set and test set. The transformations and one hot encoding creates datasets with 70 features.</p>","metadata":{"execution":{"iopub.status.busy":"2023-02-22T13:59:53.484351Z","iopub.status.idle":"2023-02-22T13:59:53.485471Z","shell.execute_reply.started":"2023-02-22T13:59:53.485235Z","shell.execute_reply":"2023-02-22T13:59:53.485259Z"}}},{"cell_type":"code","source":"# Join numeric and categorical variables to create training set\nX_train = pd.concat([X_train_num_final,X_train_cat_final], axis = 1)\n\n# Join numeric and categorical variables to create test set\nX_test = pd.concat([X_test_num_final,X_test_cat_final], axis = 1)\n\nprint('Training set shape is',X_train.shape, 'and test shape shape is',X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:15.856739Z","iopub.execute_input":"2023-02-27T09:52:15.857601Z","iopub.status.idle":"2023-02-27T09:52:15.868996Z","shell.execute_reply.started":"2023-02-27T09:52:15.857562Z","shell.execute_reply":"2023-02-27T09:52:15.86753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"font-family:monospace\">7. Model evaluation before dimension reduction</span>\n<p style = \"font-family:georgia, serif; font-size:17px\">We train and evaluate our models before we begin dimension reduction to find the accuracy when we use all the features. We will use the xgboost classifier for our evaluation since it has the highest accuracy of 0.875. This will therefore be the base value with which we will compare the accuracy of our model after dimension reduction</p>","metadata":{}},{"cell_type":"code","source":"# We instantiate different models to test their accuracy\nlogreg = LogisticRegression(max_iter = 10000)\nsvc = SVC()\nknn = KNeighborsClassifier()\nrf = RandomForestClassifier()\nxg_cl = xgb.XGBClassifier()\n\nclassifiers = [('logreg',logreg),('svc',svc),('knn',knn),('random forest',rf),('xgboost',xg_cl)]\n\n# We fit the different models and compute the accuracy\nfor clf_name, model in classifiers:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = (np.sum(y_pred == y_test))/(len(y_pred))\n    print(clf_name, accuracy)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:15.870899Z","iopub.execute_input":"2023-02-27T09:52:15.871699Z","iopub.status.idle":"2023-02-27T09:52:20.050208Z","shell.execute_reply.started":"2023-02-27T09:52:15.871662Z","shell.execute_reply":"2023-02-27T09:52:20.047624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"font-family:monospace\">8. Dimension reduction</span>\n## <span style=\"font-family:monospace; margin-left: 25px\">8.1 Feature selection with Random forests</span>\n<p style = \"font-family:georgia, serif; font-size:17px\">Random forest classifiers have a feature importances attribute that shows the contribution of each feature to the model. </p>","metadata":{"execution":{"iopub.status.busy":"2023-02-22T18:17:55.889747Z","iopub.status.idle":"2023-02-22T18:17:55.890216Z","shell.execute_reply.started":"2023-02-22T18:17:55.88998Z","shell.execute_reply":"2023-02-22T18:17:55.89Z"}}},{"cell_type":"code","source":"rf = RandomForestClassifier()\n\n# Fit the classifier\nrf.fit(X_train, y_train)\n\n# Retrieve the feature importances\nrf_importance = pd.Series(rf.feature_importances_, index = rf.feature_names_in_)\nrf_importance = rf_importance.sort_values(ascending = False)\nrf_importance.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:20.05113Z","iopub.status.idle":"2023-02-27T09:52:20.051592Z","shell.execute_reply.started":"2023-02-27T09:52:20.051378Z","shell.execute_reply":"2023-02-27T09:52:20.0514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style = \"font-family:georgia, serif; font-size:17px\">\n    \nThe top most important features according to the random forest classifier are:\n1. Rating by partner (about me) on attractiveness\n2. Whether they like their partner\n3. Rating by partner (about me) on shared interest\n4. Rating by partner (about me) at night of event on being funny\n5. Rating on your partner being funny\n\nThe least important feature is the race of the person or of their parner.\n    \n</div>","metadata":{}},{"cell_type":"code","source":"# Visualizing the feature importances\nfig = plt.figure(figsize = (10,12))\nrf_importance.plot(kind = 'barh', width =0.8)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:20.052744Z","iopub.status.idle":"2023-02-27T09:52:20.053356Z","shell.execute_reply.started":"2023-02-27T09:52:20.052996Z","shell.execute_reply":"2023-02-27T09:52:20.053024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-family:georgia, serif; font-size:17px\">The feature importance values of random forests are in percentage. Therefore, we can try and select features with a contribution of more than 1% and see if removing the features reduces noise, thus improving our model accuracy.</p>","metadata":{}},{"cell_type":"code","source":"# Selecting features \ntop_features = rf_importance[rf_importance>0.01].index\n\nxg_cl = xgb.XGBClassifier()\n\n# Fit the model\nxg_cl.fit(X_train[top_features], y_train)\n\n# Make predictions\ny_pred = xg_cl.predict(X_test[top_features])\naccuracy = (np.sum(y_pred == y_test))/(len(y_pred))\nprint(f'Dropping features from {len(rf_importance)} to {len(top_features)} give an accuracy of {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:20.054582Z","iopub.status.idle":"2023-02-27T09:52:20.055053Z","shell.execute_reply.started":"2023-02-27T09:52:20.054813Z","shell.execute_reply":"2023-02-27T09:52:20.054833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"font-family:monospace; margin-left: 25px\">8.2 Feature selection with xgboost</span>\n\n<div style = \"font-family:georgia, serif; font-size:17px\">\n    \nXgboost is used in feature selection by outputting feature importance values from a trained model. To leverage the performance and efficiency of the algorithm, the dataset is converted to a DMatrix format which is used for training. \nThe top most important features according to the xgboost classifier are:\n1. Rating by partner (about me) on attractiveness\n2. Rating of how you think your partner likes you?\n3. Rating by partner (about me) on being funny\n4. Rating by partner (about me) on shared interest\n5. How many matches do you expect to get?\n\nXgboost has the .plot_importance() method that is used to visualize the importance values of all features in a plot.\n</div>","metadata":{"execution":{"iopub.status.busy":"2023-02-22T19:15:22.259345Z","iopub.execute_input":"2023-02-22T19:15:22.25977Z","iopub.status.idle":"2023-02-22T19:15:22.269404Z","shell.execute_reply.started":"2023-02-22T19:15:22.259735Z","shell.execute_reply":"2023-02-22T19:15:22.267899Z"}}},{"cell_type":"code","source":"# Convert the data to dmatrix\ndating_matrix = xgb.DMatrix(data = X_train, label = y_train)\nparams = {'objective':'binary:logistic'}\n\n# Train data using dmatrix format\nxgb_clf = xgb.train(dtrain = dating_matrix, params = params, num_boost_round = 10)\n\n# Plot the feature importances\nfig, ax  = plt.subplots(figsize = (13,16))\nxgb.plot_importance(xgb_clf, ax = ax)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:20.057038Z","iopub.status.idle":"2023-02-27T09:52:20.057491Z","shell.execute_reply.started":"2023-02-27T09:52:20.057279Z","shell.execute_reply":"2023-02-27T09:52:20.057299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-family:georgia, serif; font-size:17px\">In xgboost, the scores of each feature can be retrieved using the .get_score() method</p>","metadata":{}},{"cell_type":"code","source":"xgb_features = pd.DataFrame(xgb_clf.get_score(), index = ['score']).T\nxgb_features['score'].sort_values(ascending=False).head(8)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:20.059624Z","iopub.status.idle":"2023-02-27T09:52:20.06015Z","shell.execute_reply.started":"2023-02-27T09:52:20.05989Z","shell.execute_reply":"2023-02-27T09:52:20.059912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-family:georgia, serif; font-size:17px\">We will evaluate the accuracy of the model with features with a score greater than 3. This selects 47 features which does not greatly affect our accuracy.</p>","metadata":{}},{"cell_type":"code","source":"xgb_cl = xgb.XGBClassifier()\n\n# Selecting features\nxgb_top_features = xgb_features[xgb_features['score'] > 3].index\n\n# Fit and predict\nxgb_cl.fit(X_train[xgb_top_features],y_train)\ny_pred = xgb_cl.predict(X_test[xgb_top_features])\naccuracy = (np.sum(y_pred == y_test))/(len(y_pred))\nprint(f'Dropping features from 70 to {len(xgb_top_features)} results in an accuracy of {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:20.061886Z","iopub.status.idle":"2023-02-27T09:52:20.062383Z","shell.execute_reply.started":"2023-02-27T09:52:20.06217Z","shell.execute_reply":"2023-02-27T09:52:20.062191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"font-family:monospace; margin-left: 25px\">8.3 Dimension reduction with Recursive Feature Elimination - RFE</span>\n\n<div style = \"font-family:georgia, serif; font-size:17px\">\n    \nRFE is feature selection algorithm that produces feature importances or feature coeffients when a model is passed to it. It fits the model and drops the weakest features, and repeats the process until the specified number of features is attained. \n\nUsing xgboost classifier, we test the accuracy of different number of features As seen below, the accuracy of a model with 70 features isn't very different with a model fitted with 40 features. In fact, reducing features from 30 to 15 has an insignificant impact on the model's accuracy. \n\nIn essence, if you are more concerned with what features play an important role in your prediction, the automatic selection of features by the recursive feature elimination method is very effective. In this circumstance, it's a tradeoff between dimensionality and accuracy. \n\n</div>","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\n\nxgb_cl = xgb.XGBClassifier()\n\nfor features in list(range(70,1,-5)):\n    # Instantiate and fit the RFE\n    rfe = RFE(estimator = xgb_cl, n_features_to_select = features)\n    rfe.fit(X_train,y_train)\n    \n    # Make predictions\n    y_pred = rfe.predict(X_test)\n    accuracy = (np.sum(y_pred == y_test))/(len(y_pred))\n    print(f'The accuracy of {features} features is {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:20.064207Z","iopub.status.idle":"2023-02-27T09:52:20.064653Z","shell.execute_reply.started":"2023-02-27T09:52:20.064449Z","shell.execute_reply":"2023-02-27T09:52:20.06447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"font-family:monospace; margin-left: 25px\">8.3.1 Selecting the features</span>\n<p style = \"font-family:georgia, serif; font-size:17px\">Suppose we want a balance between dimensionality of the dataset and the accuracy, we can select 15 features whose accuracy is 0.867. The .support_ attribute is used to give a mask with True and False values for features selected and those not selected respectively.</p>","metadata":{}},{"cell_type":"code","source":"xgb_cl = xgb.XGBClassifier()\n\n# Instatiate RFE with 15 features\nrfe = RFE(estimator = xgb.XGBClassifier(), n_features_to_select = 15)\n\n# Fit and predict\nrfe.fit(X_train,y_train)\ny_pred = rfe.predict(X_test)\naccuracy = (np.sum(y_pred == y_test))/(len(y_pred))\nprint(f'Model accuracy is {accuracy}')\n\n# Select a mask with features selected\nmask = rfe.support_\nxgb_top_15_features = X_train.columns[mask].tolist()\nprint(f'\\nTop 15 features in the xgb classifier are:\\n{xgb_top_15_features}')","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:20.065783Z","iopub.status.idle":"2023-02-27T09:52:20.066202Z","shell.execute_reply.started":"2023-02-27T09:52:20.06601Z","shell.execute_reply":"2023-02-27T09:52:20.066029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"font-family:monospace; margin-left: 25px\">8.3.2 Combining features from different models</span>\n<p style = \"font-family:georgia, serif; font-size:17px\">You can use different models and choose features with selected by all the models, and then use these features for prediction. We will select the top 30 features in three models and select common features</p>\n\n#### <span style=\"font-family:monospace; margin-left: 25px\">Selecting random forest features</span>","metadata":{"execution":{"iopub.status.busy":"2023-02-23T09:04:12.15284Z","iopub.execute_input":"2023-02-23T09:04:12.15326Z","iopub.status.idle":"2023-02-23T09:04:12.161928Z","shell.execute_reply.started":"2023-02-23T09:04:12.153226Z","shell.execute_reply":"2023-02-23T09:04:12.160658Z"}}},{"cell_type":"code","source":"rf = RandomForestClassifier()\n\nrfe = RFE(estimator = rf, n_features_to_select = 30)\nrfe.fit(X_train,y_train)\ny_pred = rfe.predict(X_test)\naccuracy = (np.sum(y_pred == y_test))/(len(y_pred))\nprint(f'Model accuracy is {accuracy}')\n\nmask_rfe = rfe.support_","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:20.068069Z","iopub.status.idle":"2023-02-27T09:52:20.068498Z","shell.execute_reply.started":"2023-02-27T09:52:20.068281Z","shell.execute_reply":"2023-02-27T09:52:20.068301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <span style=\"font-family:monospace; margin-left: 25px\">Selecting logistic Regression features</span>","metadata":{}},{"cell_type":"code","source":"logreg = LogisticRegression(max_iter =100000)\n\nrfe = RFE(estimator = logreg, n_features_to_select = 30)\nrfe.fit(X_train,y_train)\ny_pred = rfe.predict(X_test)\naccuracy = (np.sum(y_pred == y_test))/(len(y_pred))\nprint(f'Model accuracy is {accuracy}')\n\nmask_log = rfe.support_","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:20.069823Z","iopub.status.idle":"2023-02-27T09:52:20.070263Z","shell.execute_reply.started":"2023-02-27T09:52:20.07006Z","shell.execute_reply":"2023-02-27T09:52:20.07008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <span style=\"font-family:monospace; margin-left: 25px\">Selecting xgboost features</span>","metadata":{}},{"cell_type":"code","source":"xgb_cl = xgb.XGBClassifier()\n\nrfe = RFE(estimator = xgb_cl, n_features_to_select = 30)\nrfe.fit(X_train,y_train)\ny_pred = rfe.predict(X_test)\naccuracy = (np.sum(y_pred == y_test))/(len(y_pred))\nprint(f'Model accuracy is {accuracy}')\n\nmask_xgb = rfe.support_","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:20.07159Z","iopub.status.idle":"2023-02-27T09:52:20.072026Z","shell.execute_reply.started":"2023-02-27T09:52:20.071813Z","shell.execute_reply":"2023-02-27T09:52:20.071832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"font-family:monospace; margin-left: 25px\">8.3.3 Evaluating features selected by the three classifiers</span>\n<p style = \"font-family:georgia, serif; font-size:17px\">After selecting 30 features in all our models, we will combine them to select features chosen by all the models. We will use these features for our prediction. A total of 11 features were present in all the models.</p>","metadata":{}},{"cell_type":"code","source":"# Find sum of mask to identify common features \nmask_all_models = np.sum([mask_rfe, mask_log,mask_xgb], axis=0)\n\n# Select features selected by all the three models\nmask = mask_all_models == 3\nmost_important_features = X_train.columns[mask]\n\nprint(most_important_features.shape)\nprint()\nprint(most_important_features)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:20.073367Z","iopub.status.idle":"2023-02-27T09:52:20.073763Z","shell.execute_reply.started":"2023-02-27T09:52:20.073564Z","shell.execute_reply":"2023-02-27T09:52:20.073583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-family:georgia, serif; font-size:17px\">When the 11 features selected by the three models are evaluated, they give an accuracy of 0.863 with xgboost, a value not very far from 0.875 when using 70 features.</p>","metadata":{}},{"cell_type":"code","source":"xg_cl = xgb.XGBClassifier()\n\nxg_cl.fit(X_train[most_important_features], y_train)\ny_pred = xg_cl.predict(X_test[most_important_features])\naccuracy = (np.sum(y_pred == y_test))/(len(y_pred))\nprint(f'Accuracy of 11 features selected by all models is {accuracy}')    ","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:20.075589Z","iopub.status.idle":"2023-02-27T09:52:20.076289Z","shell.execute_reply.started":"2023-02-27T09:52:20.076082Z","shell.execute_reply":"2023-02-27T09:52:20.076105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"font-family:monospace; margin-left: 25px\">8.4 Dimension reduction with PCA</span>\n<p style = \"font-family:georgia, serif; font-size:17px\">PCA is another dimension-reduction technique. It has the explained_variance_ratio_ attribute that provides the variance of each feature. </p>","metadata":{"execution":{"iopub.status.busy":"2023-02-23T09:52:26.855261Z","iopub.execute_input":"2023-02-23T09:52:26.855897Z","iopub.status.idle":"2023-02-23T09:52:26.863075Z","shell.execute_reply.started":"2023-02-23T09:52:26.85586Z","shell.execute_reply":"2023-02-23T09:52:26.861379Z"}}},{"cell_type":"code","source":"pca = PCA()\npca.fit(X_train, y_train)\nvariance = pca.explained_variance_ratio_\nvariance","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:20.077749Z","iopub.status.idle":"2023-02-27T09:52:20.078199Z","shell.execute_reply.started":"2023-02-27T09:52:20.077992Z","shell.execute_reply":"2023-02-27T09:52:20.078012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-family:georgia, serif; font-size:17px\">A plot of explained variance ratio is a good indicator of the ideal number of n_components to be used in PCA. The number of components is picked from the elbow where there is an abrupt shift in explained variance. In this case, it is 11.</p>","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize = (10,6))\nsns.lineplot(variance)\nplt.grid(True)\nplt.xlabel('Number')\nplt.ylabel('Explained variance ratio')\nplt.title('A plot of explained variance')\nplt.xticks(np.arange(0,80,10), np.arange(1,81,10))\nplt.annotate('Elbow',xy =[9, 0.022], xytext = [31, 0.04], arrowprops=dict(facecolor='grey', shrink=0.05))\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:20.080352Z","iopub.status.idle":"2023-02-27T09:52:20.080744Z","shell.execute_reply.started":"2023-02-27T09:52:20.080554Z","shell.execute_reply":"2023-02-27T09:52:20.080573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-family:georgia, serif; font-size:17px\">Using the number of components from the elbow, we fit_transform the train dataset and tranform the test set. Using the xgb classifier after reducing the dimensions of data from 70 to 11 features results in an accuracy of 0.85, a slight drop from 0.87 when using 70 features.</p>","metadata":{}},{"cell_type":"code","source":"# Instantiate PCA with 11 components\npca = PCA(n_components = 11)\n\nxgb_cl = xgb.XGBClassifier()\n\n# Fit and transform the training set\nX_train_transformed = pca.fit_transform(X_train)\n\n# Transform the test set\nX_test_tranformed = pca.transform(X_test)\n\n# Fit and predict with the transformed datasets\nxgb_cl.fit(X_train_transformed,y_train)\ny_pred = xgb_cl.predict(X_test_tranformed)\naccuracy = (np.sum(y_pred == y_test))/(len(y_pred))\nprint(accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:52:20.082663Z","iopub.status.idle":"2023-02-27T09:52:20.083121Z","shell.execute_reply.started":"2023-02-27T09:52:20.082889Z","shell.execute_reply":"2023-02-27T09:52:20.082908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}